{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facebook-sdk in /home/riccardo/Scrivania/envfacebook/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: requests in /home/riccardo/Scrivania/envfacebook/lib/python3.8/site-packages (from facebook-sdk) (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/riccardo/Scrivania/envfacebook/lib/python3.8/site-packages (from requests->facebook-sdk) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/riccardo/Scrivania/envfacebook/lib/python3.8/site-packages (from requests->facebook-sdk) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/riccardo/Scrivania/envfacebook/lib/python3.8/site-packages (from requests->facebook-sdk) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/riccardo/Scrivania/envfacebook/lib/python3.8/site-packages (from requests->facebook-sdk) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install facebook-sdk\n",
    "\n",
    "import urllib3, facebook,requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "GraphAPIError",
     "evalue": "An unknown error has occurred.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGraphAPIError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-157fab50d2d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-157fab50d2d1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"345337870076487|7yqEhZoni8gpeXns3odCgM4SS4g\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfacebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcomments\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"placetopic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/envfacebook/lib/python3.8/site-packages/facebook/__init__.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, type, **args)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/search/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Scrivania/envfacebook/lib/python3.8/site-packages/facebook/__init__.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, path, args, post_args, files, method)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGraphAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGraphAPIError\u001b[0m: An unknown error has occurred."
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def main():\n",
    "    token = \"345337870076487|7yqEhZoni8gpeXns3odCgM4SS4g\"\n",
    "    graph = facebook.GraphAPI(token)\n",
    "    comments =graph.search(\"placetopic\")\n",
    "    print(comments)\n",
    "if __name__ ==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-0bec0c746a8d>, line 94)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-0bec0c746a8d>\"\u001b[0;36m, line \u001b[0;32m94\u001b[0m\n\u001b[0;31m    if v:print '\\tMESSAGE',dd[u'message'].encode('utf-8')\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "########################\n",
    "# Script to extract Facebook content\n",
    "# form public pages based on a keyword\n",
    "# search. Requires an access token\n",
    "# which is only valid for 1 month\n",
    "# Optional first arg is ID of FB page to restart from\n",
    "# Optional second arg is URL to query when restarting\n",
    "# from middway through a page's comments\n",
    "########################\n",
    "import json,requests\n",
    "import sys,csv,re,os\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "v=False\n",
    "#v=True\n",
    "# Flag for verbose printing\n",
    "\n",
    "outFile=None\n",
    "# File to deposit filtered content\n",
    "logFile=csv.writer(open('log.csv','a'),delimiter='\\t')\n",
    "# Log file for requests\n",
    "\n",
    "trashFile=csv.writer(open('trash.txt','w'),delimiter='\\t')\n",
    "# File for writing weird bits of content I don't understand yet\n",
    "\n",
    "ACCESSTOKEN=''\n",
    "# Define access token\n",
    "# Needs updating every hour :-| from https://developers.facebook.com/tools/explorer/\n",
    "# Click 'Get Access Token'\n",
    "# OR get long lasting app access token\n",
    "# Create an app at developers.facebook.com,\n",
    "# get app iD and app secret and get long lasting key\n",
    "# curl 'https://graph.facebook.com/oauth/access_token?client_id=<app_id>&client_secret=<app_secret>&grant_type=client_credentials'\n",
    "\n",
    "ACCESSTOKEN=''\n",
    "# This is long lasting app key\n",
    "\n",
    "LIMIT='5000'\n",
    "# 5000 is limit for pages\n",
    "\n",
    "QUERY='Italy'\n",
    "# Query to grab pages\n",
    "\n",
    "terms=['fish']\n",
    "\n",
    "regexString='|'.join(terms)\n",
    "matchRe=re.compile(regexString)\n",
    "# Construct regex from terms\n",
    "\n",
    "nMatches=0\n",
    "nTrash=0\n",
    "\n",
    "#################\n",
    "def logQuery(url):\n",
    "#################\n",
    "# Makes an entry in log file of URL, ID of page queries and time\n",
    "  global logFile\n",
    "\n",
    "  pageId=url.partition('graph.facebook.com/')[2]\n",
    "  pageId=pageId.partition('/posts')[0]\n",
    "\n",
    "  logFile.writerow([strftime(\"%H:%M:%S\",time.localtime()),pageId,url])\n",
    "\n",
    "#################\n",
    "def matchesQuery(text,outFile):\n",
    "#################\n",
    "# Searches each piece of content for a\n",
    "# single search term returns true/false\n",
    "# Can't use compiled regex with unicode flag\n",
    "  returnVal=False\n",
    "  res=re.search(regexString,text,re.UNICODE|re.IGNORECASE)\n",
    "  if res:\n",
    "    returnVal=True\n",
    "    outFile.writerow(['MATCH',res.group().encode('utf-8')])\n",
    "    # Log each match\n",
    "  return returnVal\n",
    "#################\n",
    "def parsePosts(rr,nPages,nPosts,category):\n",
    "#################\n",
    "# Cycles through all posts form a given FB page\n",
    "# if matching keywords, writes to file\n",
    "  global outFile\n",
    "  global logFile\n",
    "  global nMatches\n",
    "  global trashFile\n",
    "  global nTrash\n",
    "\n",
    "  for d,dd in enumerate(rr[u'data']):\n",
    "      nPosts+=1\n",
    "\n",
    "      try:\n",
    "        if v:print '\\tMESSAGE',dd[u'message'].encode('utf-8')\n",
    "      except:\n",
    "        z=0\n",
    "\n",
    "      if u'message' in dd.keys():\n",
    "    # Can we use 'story' entries?\n",
    "        message=dd['message'].encode('utf-8')\n",
    "        outLine=['POST',dd[u'id'],dd[u'created_time']]\n",
    "        outLine.append(message.replace('\\n',' | '))\n",
    "        outLine.append(category)\n",
    "        if v:print 'MATCHES?'\n",
    "        if matchesQuery(dd['message'],outFile):\n",
    "          outFile.writerow([o for o in outLine])\n",
    "          nMatches+=1\n",
    "      else:\n",
    "        if v:print '!!! NO MESSAGE',dd.keys()\n",
    "\n",
    "      if v: print 'COMMENTS?'\n",
    "\n",
    "      if u'comments' in dd.keys():\n",
    "        if dd[u'type'] in [u'photo',u'swf',u'link',u'status',u'video']:\n",
    "#          print 'HAS COMMENTS',dd[u'type'],dd[u'link'],dd.keys()\n",
    "          if u'link' in dd.keys():trashFile.writerow([dd[u'type'],dd[u'link'].encode('utf-8')])\n",
    "          else:trashFile.writerow([dd[u'type']])\n",
    "#          sys.exit(1)\n",
    "          # Keep track of photo,links,statuses etc which DO have comments (many don't)\n",
    "        for c in dd['comments']['data']:\n",
    "          if v:print '\\t\\tCOMMENT',c['message']\n",
    "          message=c['message'].encode('utf-8')\n",
    "          outLine=['COMMENT',c[u'id'],c[u'created_time']]\n",
    "          outLine.append(re.sub('\\n',' | ',message))\n",
    "          outLine.append(category)\n",
    "          if matchesQuery(outLine[-2],outFile):\n",
    "            outFile.writerow([o for o in outLine])\n",
    "            nMatches+=1\n",
    "      ## If not comments, catch other possible types\n",
    "      ## These all seem to consistently not have any comments\n",
    "      ## Tried querying many different types across many different pages\n",
    "      ## Seems not due to privacy settings as comments possible in browser\n",
    "      ## Also swf,question\n",
    "      #################################\n",
    "      elif dd[u'type']==u'video':\n",
    "        outLine=['VIDEO',dd[u'id'],dd[u'created_time']]\n",
    "        contentString=''\n",
    "        if u'link' in dd.keys(): trashFile.writerow([dd[u'link'].encode('utf-8')])\n",
    "        trashFile.writerow(dd.keys())\n",
    "\n",
    "        for k in [u'description',u'message',u'caption']:\n",
    "          if k in dd.keys():\n",
    "            contentString+='|'+dd[k].replace('\\n','|')\n",
    "        contentString=contentString.encode('utf-8')\n",
    "        if matchesQuery(contentString,outFile):\n",
    "          outLine.append(contentString)\n",
    "          outFile.writerow(outLine)\n",
    "          nMatches+=1\n",
    "      #################################\n",
    "      elif dd[u'type']==u'status':\n",
    "        # message\n",
    "        # likes\n",
    "        outLine=['STATUS',dd[u'id'],dd[u'created_time']]\n",
    "        contentString=''\n",
    "        if u'link' in dd.keys(): trashFile.writerow([dd[u'link'].encode('utf-8')])\n",
    "        trashFile.writerow(dd.keys())\n",
    "\n",
    "        for k in [u'message']:\n",
    "          if k in dd.keys():\n",
    "            contentString+='|'+dd[k].replace('\\n','|')\n",
    "        contentString=contentString.encode('utf-8')\n",
    "        if matchesQuery(contentString,outFile):\n",
    "          outLine.append(contentString)\n",
    "          outFile.writerow(outLine)\n",
    "          nMatches+=1\n",
    "      #################################\n",
    "      elif dd[u'type']==u'photo':\n",
    "        # picture,message\n",
    "        # likes\n",
    "        outLine=['PHOTO',dd[u'id'],dd[u'created_time']]\n",
    "        contentString=''\n",
    "        if u'link' in dd.keys(): trashFile.writerow([dd[u'link'].encode('utf-8')])\n",
    "        trashFile.writerow(dd.keys())\n",
    "\n",
    "        for k in [u'picture',u'message']:\n",
    "          if k in dd.keys():\n",
    "            contentString+='|'+dd[k].replace('\\n','|')\n",
    "        contentString=contentString.encode('utf-8')\n",
    "        if matchesQuery(contentString,outFile):\n",
    "          outLine.append(contentString)\n",
    "          outFile.writerow(outLine)\n",
    "          nMatches+=1\n",
    "      #################################\n",
    "      elif dd[u'type']==u'link':\n",
    "        # description,message\n",
    "        # likes\n",
    "        outLine=['LINK',dd[u'id'],dd[u'created_time']]\n",
    "        contentString=''\n",
    "        if u'link' in dd.keys(): trashFile.writerow([dd[u'link'].encode('utf-8')])\n",
    "        trashFile.writerow(dd.keys())\n",
    "\n",
    "        for k in [u'description',u'message']:\n",
    "          if k in dd.keys():\n",
    "            contentString+='|'+dd[k].replace('\\n','|')\n",
    "        contentString=contentString.encode('utf-8')\n",
    "        if matchesQuery(contentString,outFile):\n",
    "          outLine.append(contentString)\n",
    "          outFile.writerow(outLine)\n",
    "          nMatches+=1\n",
    "      #################################\n",
    "      else:\n",
    "        if v:print '!!! NO COMMENTS',dd.keys()\n",
    "        #print dd\n",
    "#        nTrash+=1\n",
    "#        print 'ADDING TO TRASH FILE',nTrash\n",
    "#        print dd.keys(),\n",
    "#        print 'TYPE',dd['type']\n",
    "#        print dd[u'id']\n",
    "#        if u'link' in dd.keys():print dd[u'link']\n",
    "#        if 'message' in dd.keys():print 'MESSAGE',dd['message'].replace('\\n','|').encode('utf-8'),dd['type']\n",
    "#        json.dump(dd,trashFile,indent=2)\n",
    "#        sys.exit(1)\n",
    "\n",
    "      if v:print '+++++++++++++++++++'\n",
    "  if v:print ''\n",
    "  nPages+=1\n",
    "\n",
    "  return nPages,nPosts\n",
    "  '''\n",
    "  except:\n",
    "    print 'MISSING data KEY',rr.keys()\n",
    "    print rr[u'error_code'],rr[u'error_msg']\n",
    "    outFile.writerow(['MISSING DATA'])\n",
    "#    sys.exit(1)\n",
    "  '''\n",
    "########################\n",
    "def main():\n",
    "########################\n",
    "  global outFile\n",
    "  restartOffset=0\n",
    "  nPostsTotal=0\n",
    "  # Counts total number of unfiltered posts considered\n",
    "  nMatchesTotal=0\n",
    "  global nMatches\n",
    "\n",
    "  startTime=time.localtime()\n",
    "###################################\n",
    "# Parse args\n",
    "  if len(sys.argv)==2:\n",
    "    restartId=sys.argv[1]\n",
    "#    outFile=csv.writer(open('out_'+QUERY.encode('utf-8')+'.csv','a'),delimiter='\\t')\n",
    "    skip=True\n",
    "    commentsPageSkip=False\n",
    "    print '******APPENDING TO FILE','out_'+QUERY+'.csv'\n",
    "    print '******RESTARTING FROM PAGE',restartId\n",
    "    raw_input('IS THIS OK?')\n",
    "    outFile=csv.writer(open('out_'+QUERY+'.csv','a'),delimiter='\\t')\n",
    "    restartCommentsPage=None\n",
    "  elif len(sys.argv)==3:\n",
    "    restartId=sys.argv[1]\n",
    "    restartCommentsPage=sys.argv[2]\n",
    "#    outFile=csv.writer(open('out_'+QUERY.encode('utf-8')+'.csv','a'),delimiter='\\t')\n",
    "    skip=True\n",
    "    commentsPageSkip=True\n",
    "    print '******APPENDING TO FILE','out_'+QUERY+'.csv'\n",
    "    print '******RESTARTING FROM POSTS PAGE',restartCommentsPage\n",
    "    raw_input('IS THIS OK?')\n",
    "    outFile=csv.writer(open('out_'+QUERY+'.csv','a'),delimiter='\\t')\n",
    "  else:\n",
    "    print '******OPENING OUTFILE','out_'+QUERY+'.csv'\n",
    "    if 'out_'+QUERY+'.csv' in os.listdir('.'):print '!!!!!WILL OVERWRITE'\n",
    "    raw_input('IS THIS OK?')\n",
    "    outFile=csv.writer(open('out_'+QUERY+'.csv','w'),delimiter='\\t')\n",
    "    skip=False\n",
    "    commentsPageSkip=False\n",
    "    restartCommentsPage=None\n",
    "    restartId=-9999\n",
    "  # restartCommentsPage is FB page to resume from\n",
    "  # restartId is ID of FB page to resume from\n",
    "  # skip is flag to skip FB pages until restartId is found\n",
    "  # commentsPageSkip is flag to skip pages of comments on a\n",
    "  # FB page matching restartId until restartCommentsPage is found\n",
    "###################################\n",
    "  tempUrl='https://graph.facebook.com/search?q='+QUERY+'&limit='+LIMIT+'&type=page&access_token='+ACCESSTOKEN\n",
    "  r=requests.get(tempUrl).json()\n",
    "  logQuery(tempUrl)\n",
    "# Get all pages matching QUERY\n",
    "\n",
    "#  LIMIT=5000\n",
    "  # Make limit higher once we start to look at comments not pages???\n",
    "\n",
    "  if not 'data' in r.keys():\n",
    "    print 'EXPIRED????',r\n",
    "    sys.exit(1)\n",
    "################################################\n",
    "  for p,page in enumerate(r[u'data']):\n",
    "# Each page has 'category','name','id'\n",
    "    errorSkip=False\n",
    "    nError=0\n",
    "\n",
    "    try:\n",
    "      print 'PAGE #',p,'('+str(len(r[u'data']))+')',page[u'name'],page[u'category'],page[u'id'],strftime(\"%H:%M:%S\", time.localtime())\n",
    "    except:\n",
    "      print '!!!!!!!PAGE ERROR'\n",
    "\n",
    "    if page[u'id']==restartId:\n",
    "      skip=False\n",
    "      print 'RESTARTING....'\n",
    "\n",
    "    if not skip:\n",
    "      tempUrl='https://graph.facebook.com/'+page[u'id']+'/posts?'+'&limit='+LIMIT+'&access_token='+ACCESSTOKEN\n",
    "      logQuery(tempUrl)\n",
    "      rr=requests.get(tempUrl).json()\n",
    "      # Try to get the posts\n",
    "\n",
    "      while u'error' in rr.keys() or u'error_msg' in rr.keys():\n",
    "        if (u'error' in rr.keys() and u'code' in rr[u'error'].keys() and rr[u'error'][u'code'] in [1,2]) or u'error_msg' in rr.keys():\n",
    "        # API error\n",
    "          print 'API ERROR: SLEEPING....'\n",
    "          print rr\n",
    "          time.sleep(60)\n",
    "          print 'RETRYING (1)'\n",
    "          nError+=1\n",
    "          if nError==10:\n",
    "            print nError,'ERRORS - SKIPPING'\n",
    "            errorSkip=True\n",
    "            break\n",
    "        else:\n",
    "        # TOKEN ERROR\n",
    "          print '********ERROR',rr[u'error']\n",
    "          sys.exit(1)\n",
    "        tempUrl='https://graph.facebook.com/'+page[u'id']+'/posts?'+'&limit='+LIMIT+'&access_token='+ACCESSTOKEN\n",
    "        rrtemp=requests.get(tempUrl)\n",
    "        print 'rrtemp',rrtemp,rrtemp.text\n",
    "        rr=rrtemp.json()\n",
    "        logQuery(tempUrl)\n",
    "        # Try to get the posts again\n",
    "\n",
    "      nPages=0\n",
    "      nError=0\n",
    "      nPosts=0\n",
    "      nMatches=0\n",
    "\n",
    "      outFile.writerow(['PAGE',page[u'id'],page[u'name'].encode('utf-8'),page[u'category'].encode('utf-8')])\n",
    "\n",
    "      if not errorSkip and not commentsPageSkip:\n",
    "      # If API has caused 3 errors, skip\n",
    "      # Or if restarting from a later comments page, skip\n",
    "        errorSkip=False\n",
    "        nPages,nPosts=parsePosts(rr,nPages,nPosts,page[u'category'].encode('utf-8'))\n",
    "\n",
    "      while 'paging' in rr.keys() and not errorSkip and not commentsPageSkip:\n",
    "\n",
    "        if v:print 'LOADING',rr[u'paging'][u'next']\n",
    "\n",
    "        rrrRaw=requests.get(rr[u'paging'][u'next'])\n",
    "        logQuery(rr[u'paging'][u'next'])\n",
    "\n",
    "        if rr['paging']['next']==restartCommentsPage and restartCommentsPage:\n",
    "          commentsPageSkip=False\n",
    "          print '**********MATCHED RESTART PAGE - RESUMING PARSING COMMENTS'\n",
    "        # If we want to restart from last page\n",
    "        elif restartCommentsPage and restartId==page['id']:\n",
    "          print '**********DIDNT MATCH COMMENTS RESTART PAGE'\n",
    "          restartOffset+=1\n",
    "\n",
    "        try:\n",
    "          rrr=rrrRaw.json()\n",
    "        except:\n",
    "          print 'JSON ERROR', rrrRaw.status_code\n",
    "\n",
    "        while u'error' in rrr.keys() or u'error_msg' in rrr.keys():\n",
    "\n",
    "          if u'error' in rrr.keys() or u'error_msg' in rrr.keys():\n",
    "          # API error\n",
    "            print 'API ERROR: SLEEPING....'\n",
    "            print rrr,rrrRaw,rrrRaw.status_code,rrrRaw.text\n",
    "            print rr[u'paging'][u'next']\n",
    "            time.sleep(10)\n",
    "            print 'RETRYING'\n",
    "            nError+=1\n",
    "            if nError==10:\n",
    "              print nError,'ERRORS - SKIPPING'\n",
    "              errorSkip=True\n",
    "              break\n",
    "          else:\n",
    "          # TOKEN ERROR ?\n",
    "            print '********ERROR',rrr\n",
    "            sys.exit(1)\n",
    "\n",
    "          rrr=requests.get(rr[u'paging'][u'next'])\n",
    "#          print 'rrr',rrr,rrr.text,rrr.status_code\n",
    "#          print\n",
    "          rrr=rrr.json()\n",
    "          logQuery(rr['paging']['next'])\n",
    "        # Try to get the posts again\n",
    "        # if the API doesn't respond\n",
    "\n",
    "        if not commentsPageSkip:\n",
    "          if v:\n",
    "            print '# COMMENTS PAGES',nPages,'# POSTS',nPosts,'# MATCHES',nMatches,strftime(\"%H:%M:%S\", time.localtime()),\n",
    "            if not restartOffset==0:\n",
    "              print '# OFFSET',restartOffset\n",
    "            else:\n",
    "              print ''\n",
    "\n",
    "        if (not errorSkip and not commentsPageSkip) and not skip:\n",
    "        # If API has caused 10 errors in a row\n",
    "        # Or if restarting from a later page of comments\n",
    "        # or if not already found restart page, skip\n",
    "          nPages,nPosts=parsePosts(rrr,nPages,nPosts,page[u'category'].encode('utf-8'))\n",
    "        else:\n",
    "          print '************NOT PARSING POSTS',errorSkip,commentsPageSkip\n",
    "          print 'BREAKING'\n",
    "          break\n",
    "        rr=rrr\n",
    "      print '# COMMENTS PAGES',nPages,'# POSTS',nPosts,'# MATCHES',nMatches,strftime(\"%H:%M:%S\", time.localtime())\n",
    "      if not restartOffset==0:\n",
    "        print '# OFFSET',restartOffset\n",
    "\n",
    "      outFile.writerow(['PAGE TOTALS',str(nPages),str(nPosts),str(nMatches)])\n",
    "      outFile.writerow(['RUNNING PAGE TOTALS',p,str(nPostsTotal),str(nMatchesTotal)])\n",
    "      nPostsTotal+=nPosts\n",
    "      nMatchesTotal+=nMatches\n",
    "      print 'TOTAL SO FAR #POSTS',nPostsTotal,'#MATCHES',nMatchesTotal\n",
    "      print '-----------'\n",
    "      restartOffset=0\n",
    "    else:\n",
    "      print 'SKIPPING.....',nPostsTotal\n",
    "  print 'FINISHED',strftime(\"%H:%M:%S\",startTime),'-',strftime(\"%H:%M:%S\",time.localtime())\n",
    "#####\n",
    "if __name__=='__main__':\n",
    "#####\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
